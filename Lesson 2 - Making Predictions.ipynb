{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 2 - Making Predictions\n",
    "In this workbook we will define and train a model to make predictions. We will use one of the standard Machine Learning Datasets (\"Auto-MPG\" dataset from UCI https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data).\n",
    "\n",
    "The Prediction Problem we will aim to solve is to predict the Miles-Per-Gallon we can expect from a vehicle based on some features of the car (such as number of cylinders, age of the car, the weight of the car). It is a toy dataset (but based on real data about cars from the 1970s and 1980s).\n",
    "\n",
    "The key topics we will cover in this lesson are:\n",
    "- Approximating a Prediction using Deep Learning\n",
    "- Data Overfitting - a common problem to watch out for in Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing some packages\n",
    "We are using the Python programming language and a set of Machine Learning packages - Importing packages for use is a common task. For this workshop you don't really need to pay that much attention to this step (but you do need to execute the cell) since we are focusing on building models. However the following is a description of what this cell does that you can read if you are interested.\n",
    "\n",
    "### Description of imports (Optional)\n",
    "You don't need to worry about this code as this is not the focus on the workshop but if you are interested in what this next cell does, here is an explaination.\n",
    "\n",
    "|Statement|Meaning|\n",
    "|---|---|\n",
    "|__import os__|This is a standard Python library to work with the file system|\n",
    "|__import tensorflow as tf__ |Tensorflow (from Google) is our main machine learning library and we performs all of the various calculations for us and so hides much of the detailed complexity in Machine Learning. This _import_ statement makes the power of TensorFlow available to us and for convience we will refer to it as __tf__ |\n",
    "|__from tensorflow import keras__ |Tensorflow is quite a low level machine learning library which, while powerful and flexible can be confusing so instead we use another higher level framework called Keras to make our machine learning models more readable and easier to build and test. This _import_ statement makes the Keras framework available to us.|\n",
    "|__import numpy as np__ |Numpy is a Python library for scientific computing and is commonly used for machine learning. This _import_ statement makes the Keras framework available to us.|\n",
    "|__import pandas as pd__|Pandas is a library that helps us manipulate tables of data. We import this|\n",
    "|__import seaborn as sns__|Seaborn is a library that created very useful visualisations|\n",
    "|__import matplotlib.pyplot as plt__ |To visualise what is happening in our network we will use a set of graphs and MatPlotLib is the standard Python library for producing Graphs so we __import__ this to enable us to make pretty graphs.|\n",
    "|__%matplotlib inline__| this is a Jupyter Notebook __magic__ commmand that tells the workbook to produce any graphs as part of the workbook and not as pop-up window.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "print(\"TensorFlow version is \", tf.__version__)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "The following cell contains a set of helper functions that makes our models a little clearer. We will not be going through these functions (since they require Python knowlege) so just make sure you have run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData():\n",
    "  data_file = keras.utils.get_file(fname=\"auto-mpg.data\", origin=\"https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\")\n",
    "  # The data has the following coluns\n",
    "  column_names = ['MPG','Cylinders','Displacement','Horsepower','Weight',\n",
    "                'Acceleration', 'Model Year', 'Origin Country']\n",
    "  # Read in the file into a Pandas Dataset\n",
    "  raw_dataset = pd.read_csv(data_file, names=column_names,\n",
    "                      na_values = \"?\", comment='\\t',\n",
    "                      sep=\" \", skipinitialspace=True)\n",
    "  # We will discard any rows that contain data with missing values\n",
    "  raw_dataset = raw_dataset.dropna()\n",
    "  return raw_dataset\n",
    "\n",
    "def preprocessCategoricalData(data):\n",
    "    # Use One-Hot-Encoding for categorical data\n",
    "    origin = data.pop('Origin Country')\n",
    "    data['USA'] = (origin == 1)*1.0\n",
    "    data['Europe'] = (origin == 2)*1.0\n",
    "    data['Japan'] = (origin == 3)*1.0\n",
    "    return data\n",
    "    \n",
    "\n",
    "def splitDataset(dataset):\n",
    "  # Split the data into Train and Test Sets\n",
    "  # We randomly select 80% of the records for our Training Dataset\n",
    "  train_dataset = dataset.sample(frac=0.8,random_state=0)\n",
    "  # We then use the remaining records as our Test Dataset\n",
    "  test_dataset = dataset.drop(train_dataset.index)\n",
    "\n",
    "  # Extract the labels for training and testing\n",
    "  train_labels = train_dataset.pop('MPG')\n",
    "  test_labels = test_dataset.pop('MPG')\n",
    "\n",
    "  return train_dataset, train_labels, test_dataset, test_labels\n",
    "\n",
    "def displayPairPlots(data, labels):\n",
    "  dataset = data.copy()\n",
    "  dataset['MPG'] = labels\n",
    "  sns.pairplot(dataset, diag_kind=\"kde\")\n",
    "\n",
    "def norm(x, train_stats):\n",
    "  return (x - train_stats['mean']) / train_stats['std']\n",
    "\n",
    "def normaliseData(train_dataset, test_dataset):\n",
    "  train_stats = train_dataset.describe()\n",
    "  train_stats = train_stats.transpose() \n",
    "\n",
    "  normed_train_data = norm(train_dataset, train_stats)\n",
    "  normed_test_data = norm(test_dataset, train_stats)\n",
    "\n",
    "  return normed_train_data, normed_test_data\n",
    "\n",
    "def plotHistory(history):\n",
    "  hist = pd.DataFrame(history.history)\n",
    "  hist['epoch'] = history.epoch\n",
    "\n",
    "  plt.figure()\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Mean Abs Error [MPG]')\n",
    "  plt.plot(hist['epoch'], hist['mean_absolute_error'],\n",
    "           label='Train Error')\n",
    "  plt.plot(hist['epoch'], hist['val_mean_absolute_error'],\n",
    "           label = 'Val Error')\n",
    "  plt.ylim([0,5])\n",
    "  plt.legend()\n",
    "\n",
    "  plt.figure()\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Mean Square Error [$MPG^2$]')\n",
    "  plt.plot(hist['epoch'], hist['mean_squared_error'],\n",
    "           label='Train Error')\n",
    "  plt.plot(hist['epoch'], hist['val_mean_squared_error'],\n",
    "           label = 'Val Error')\n",
    "  plt.ylim([0,20])\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "\n",
    "def displayScatter(expected, predictions):\n",
    "  plt.scatter(expected, predictions)\n",
    "  plt.xlabel('True Values [MPG]')\n",
    "  plt.ylabel('Predictions [MPG]')\n",
    "  plt.axis('equal')\n",
    "  plt.axis('square')\n",
    "  plt.xlim([0,plt.xlim()[1]])\n",
    "  plt.ylim([0,plt.ylim()[1]])\n",
    "  _ = plt.plot([-100, 100], [-100, 100])\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "# Display training progress by printing a single dot for each completed epoch\n",
    "class PrintDot(keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs):\n",
    "    if epoch % 100 == 0: print('')\n",
    "    print('.', end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the Auto-MPG dataset\n",
    "The data we want to use is hosted by UCI and so we have to download it into our workspace before we can use it. We have created a function in _lesson2.py_ to import the data ready for you to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = getData()\n",
    "\n",
    "dataset = raw_dataset.copy()\n",
    "\n",
    "dataset.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "In the data above we have two types of data:\n",
    "- Numerical Data - data that describes some quantity of measurement\n",
    "- Categorical Data - data that describes belonging to a Category\n",
    "\n",
    "Which columns do you think are Categorical data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to perform some pre-processing on the data to encode Categorical Data \n",
    "# in a way that supports learning\n",
    "dataset = preprocessCategoricalData(dataset)\n",
    "dataset.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Training and Testing Datasets\n",
    "Currently all our data is in one set but ideally we want to have the following datasets:\n",
    "\n",
    "|Data set| Purpose|\n",
    "|---|---|\n",
    "|training|We use this to train our model|\n",
    "|testing|We use this to test our model - this data is never used for training|\n",
    "\n",
    "In addition, we need to split out our target values (i.e. the values we want to predict). If you remember from lesson 1 we had our training data (miles_travelled) and a set of expected values (expected_cost). We need to to the same for this dataset.\n",
    "\n",
    "We have created a function in _lesson2.py_ to do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, train_labels, test_dataset, test_labels = splitDataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualisation\n",
    "It is useful to be able to visualise the data we have, in particular we want to check that there appears to be some correlation between the different data attributes and our target (MPG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = train_dataset.copy()\n",
    "temp['MPG'] = train_labels\n",
    "temp.tail()\n",
    "displayPairPlots(train_dataset[[\"Cylinders\", \"Acceleration\", \"Weight\"]], train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalise the Data\n",
    "We have numerical data that has very different ranges; for example compare the values for _Acceleration_ and _Weight_.\n",
    "\n",
    "Often, having such differing ranges causes problems for machine learning so we __Normalise__ the data; this is a mathematical operation that preserves the relationship between datapoints but brings them into similar ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normed_train_data, normed_test_data = normaliseData(train_dataset, test_dataset)\n",
    "\n",
    "normed_test_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining our Model\n",
    "Now that we have the data in reasonable shape we need to define our Deep Learning Model. We will use the __Keras API__ to define a model consisting of a series of __Dense__ layers; these are Full-Connected layers where each unit in the layer is connected to each and every other unit in the next layer.\n",
    "\n",
    "The layers will be in sequence such that the order we add them will be the order that they are stacked in the Network and all data goes through the layers in sequence.\n",
    "\n",
    "We start of by defining our model to be a Sequence Model with the following\n",
    "\n",
    "`model = keras.Sequential()`\n",
    "\n",
    "We can then add layers to our model using:\n",
    "\n",
    "`model.add(Dense(units=128, activation = tf.nn.relu))`\n",
    "\n",
    "where _units_ is the number of units in that layer and _activation_ is the type of activation function the layer will use.\n",
    " - Note: we are not covering the details of activation functions in this workshop so just go with the provided activation function.\n",
    "\n",
    "We can add as many layers as we want and for this initial model we will have 3 layers:\n",
    "- Input Layer\n",
    "- 1 Hidden Layer\n",
    "- 1 Output Layer\n",
    "\n",
    "Once defined we will _compile_ our model with an _Optimizer_ and _Loss Function_ and we are ready to go.\n",
    "- Note that in this workshop we will not go into detail about Optimizers and Loss Functions so just go with the provided values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModel():\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    # Input Layer\n",
    "    model.add( keras.layers.Dense(units=128, activation=tf.nn.relu, input_shape=[len(train_dataset.keys())]) )\n",
    "\n",
    "    # Hidden Layer\n",
    "    model.add( keras.layers.Dense(units=64, activation=tf.nn.relu) )\n",
    "\n",
    "    # Output Layer - we want a single value to be output so our output layer has a single unit\n",
    "    model.add( keras.layers.Dense(1) )\n",
    "\n",
    "\n",
    "    # We now compile our model with Loss Function and an Optimizer\n",
    "    optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
    "\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                    optimizer=optimizer,\n",
    "                    metrics=['mean_absolute_error', 'mean_squared_error'])\n",
    "    return model\n",
    "\n",
    "# Display a summary of the model\n",
    "model = createModel()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "We are now ready to start training our model to do this we use the _Keras fit_ function, which takes a series of options that control how learning is to progress. \n",
    "\n",
    "In our example we are providing the _fit_ function with our training data and our training labels (expected values). \n",
    "\n",
    "We are also telling the Fit function to run for 1,000 epochs (iterations) so it will attempt to learn from the data 1,000 times. This is the _epochs_ option.\n",
    "\n",
    "We are also telling the fit function to take 20% of the training data and use it to validate how well the training is going. What this means is that the _fit_ function will train on 80% of the training data and after each epoch it will assess how well the model is doing against the 20% it has retained (and not trained on).\n",
    "\n",
    "This is known as _Validation Accuracy_ and is a reasonable measure of how well our model is learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1000\n",
    "\n",
    "history = model.fit(\n",
    "  normed_train_data, train_labels,\n",
    "  epochs=EPOCHS, validation_split = 0.2, verbose=0,\n",
    "  callbacks=[PrintDot()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see how we have done\n",
    "The model has trained for 1,000 epochs but how well has it done?\n",
    "\n",
    "One way is to look at the Errors Levels during Training and Validation; these are the amount by which our model predictions were wrong.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotHistory(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's going wrong?\n",
    "If we look at the graphs we can see that as the number of epochs increases, the difference between the Training and Validation Errors increases.\n",
    "\n",
    "This is known as __Overfitting__ and is a problem in Machine Learning where a model becomes too good at predicting the values in the Training set and is unable to generalise to values not in the dataset.\n",
    "\n",
    "For our model to be of any use we want to avoid Overfitting - one appraoch is to stop training earlier - we trained for 1,000 epochs but probably didn't need to.\n",
    "\n",
    "### Exercise\n",
    "How many Epochs do you think we should have trained for to produce a __Low__ Validation Loss while minimising the difference between the Training Error and the Validation Error?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping and Overfitting\n",
    "One approach to overfitting is to stop training soon as it looks like our Validation Loss has stagnated. We could just change the number of _epochs_ but instead we will use a _Keras_ feature that detects this for us and stops the training early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new instace of our model\n",
    "model = createModel()\n",
    "\n",
    "# The patience parameter is the amount of epochs to check for improvement\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "history = model.fit(normed_train_data, train_labels, epochs=EPOCHS,\n",
    "                    validation_split = 0.2, verbose=0, callbacks=[early_stop, PrintDot()])\n",
    "\n",
    "plotHistory(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, mae, mse = model.evaluate(normed_test_data, test_labels, verbose=0)\n",
    "\n",
    "print(\"Testing set Mean Abs Error: {:5.2f} MPG\".format(mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model.predict(normed_test_data).flatten()\n",
    "\n",
    "displayScatter(test_labels, test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Can you improve on this model?\n",
    "\n",
    "Work in your groups and define a set of model layers; try:\n",
    "- Models with 1, 2, 3, 4 or 5 Hidden Layers\n",
    "- Set the number of units in each layer in ascenend, decending and mixed values. For example\n",
    "    - 32, 64 and 128\n",
    "    - 128, 64 and 32\n",
    "    - 128, 32, 64\n",
    "\n",
    "Then each team member should implement one of the models and train it; once training is complete discuss the following within your team.\n",
    " - Which of your models produced the best result?\n",
    " - Which models produced the worst results?\n",
    " - Based on your results and the models you picked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = keras.Sequential()\n",
    "\n",
    "# YOUR CHANGES START HERE\n",
    "# TODO:\n",
    "#   - Decide how many nodes you want in your input layer (or leave it as is)\n",
    "#   - Decide how many hidden layers you want in your model\n",
    "#   - For each layer in your model decide how many nodes and change the value '64' to this value. \n",
    "#       Common values are 32, 64, 128, 256\n",
    "\n",
    "# Input Layer - Todo: if you want to, try changing  the number of units in the input layer\n",
    "my_model.add(keras.layers.Dense(units=128, activation=tf.nn.relu, input_shape=[len(train_dataset.keys())]))\n",
    "\n",
    "# Hidden Layers - Todo: copy the line below to create additional layers\n",
    "#                       and update the number of units in each layer\n",
    "my_model.add(keras.layers.Dense(units=64, activation=tf.nn.relu))\n",
    "\n",
    "# YOUR CHANGES END HERE\n",
    "\n",
    "# Output Layer\n",
    "my_model.add(keras.layers.Dense(1))\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "my_model.compile(loss='mean_squared_error',\n",
    "                optimizer=tf.keras.optimizers.RMSprop(0.001),\n",
    "                metrics=['mean_absolute_error', 'mean_squared_error'])\n",
    "\n",
    "my_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The patience parameter is the amount of epochs to check for improvement\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "history = my_model.fit(normed_train_data, train_labels, epochs=EPOCHS,\n",
    "                    validation_split = 0.2, verbose=0, callbacks=[early_stop, PrintDot()])\n",
    "\n",
    "plotHistory(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, mae, mse = my_model.evaluate(normed_test_data, test_labels, verbose=0)\n",
    "\n",
    "print(\"Testing set Mean Abs Error: {:5.2f} MPG\".format(mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model.predict(normed_test_data).flatten()\n",
    "\n",
    "displayScatter(test_labels, test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "In this workbook you have seen how it is possible to analyse data, both categorical and numerical, to predict some other value.\n",
    "\n",
    "Discuss in your groups how you might use Prediction in your work (or other context). Consider:\n",
    "- Where would the ability to predict provide some aid?\n",
    "- What data could you collect to build a predictive model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook introduced a few techniques to handle a regression problem.\n",
    "\n",
    "* Mean Squared Error (MSE) is a common loss function used for regression problems (different loss functions are used for classification problems).\n",
    "* Similarly, evaluation metrics used for regression differ from classification. A common regression metric is Mean Absolute Error (MAE).\n",
    "* When numeric input data features have values with different ranges, each feature should be scaled independently to the same range.\n",
    "* If there is not much training data, one technique is to prefer a small network with few hidden layers to avoid overfitting.\n",
    "* Early stopping is a useful technique to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
