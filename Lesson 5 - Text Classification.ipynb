{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 5 - Text Classification\n",
    "In the previous 2 lessons we have seen how a machine can take a digital image and learn some form of classification of the image. In this lesson we will look at how a machine can learn from Textual data to perform some form of analysis and classification.\n",
    "\n",
    "The new concepts in this lesson are:\n",
    "- Learning from Text\n",
    "- Word Embeddings\n",
    "- Sentiment Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing some packages\n",
    "We are using the Python programming language and a set of Machine Learning packages - Importing packages for use is a common task. For this workshop you don't really need to pay that much attention to this step (but you do need to execute the cell) since we are focusing on building models. However the following is a description of what this cell does that you can read if you are interested.\n",
    "\n",
    "### Description of imports (Optional)\n",
    "You don't need to worry about this code as this is not the focus on the workshop but if you are interested in what this next cell does, here is an explaination.\n",
    "- __import tensorflow as tf__ - Tensorflow (from Google) is our main machine learning library and we performs all of the various calculations for us and so hides much of the detailed complexity in Machine Learning. This _import_ statement makes the power of TensorFlow available to us and for convience we will refer to it as __tf__\n",
    "- __from tensorflow import keras__ - Tensorflow is quite a low level machine learning library which, while powerful and flexible can be confusing so instead we use another higher level framework called Keras to make our machine learning models more readable and easier to build and test. This _import_ statement makes the Keras framework available to us.\n",
    "- __import numpy as np__ - Numpy is a Python library for scientific computing and is commonly used for machine learning. This _import_ statement makes the Keras framework available to us.\n",
    "- __import matplotlib.pyplot as plt__ - To visualise what is happening in our network we will use a set of graphs and MatPlotLib is the standard Python library for producing Graphs so we __import__ this to enable us to make pretty graphs.\n",
    "- __%matplotlib inline__ - this is a Jupyter Notebook __magic__ commmand that tells the workbook to produce any graphs as part of the workbook and not as pop-up window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "import lesson5\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About Our Dataset\n",
    "In this lesson we will be using a Standard Text Database called the IMDB Reviews Database. This contains a large number of reviews from the Internet Movie Database site about a range of files.\n",
    "\n",
    "This is a standard dataset used in Machine Learning for Natural Language Processing and _Keras_ provides us with easy access to this data. Each Movie Review has a Sentiment attached either Positive or Negative. We want to train a Classifer to estimate whether it things a provide review is Positive or Negative or some where in between.\n",
    "\n",
    "\n",
    "### Discussion Point\n",
    "Consider the review:\n",
    "\n",
    "__\"Overall the movie was utterly awsome although some of the scenes could be better litter. The story and cast were captivating. It's up there in my top 100 movies\"__\n",
    "\n",
    "Based on what you known about Deep Learning already, how do we make this review suitable for a learning model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "imdb = keras.datasets.imdb\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
    "print(\"Training entries: {}, labels: {}\".format(len(train_data), len(train_labels)))\n",
    "# A dictionary mapping words to an integer index\n",
    "word_index, reverse_word_index = lesson5.getWordIndex(imdb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's look at a review\n",
    "The IMDB dataset in Keras is already pre-encoded and ready for use. So we can print out a review as the Computer will work with it and then decode this to show the human readable review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Machine Readable Review: \\n{}\".format(train_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Human Readable Review: \\n{}\".format(lesson5.decodeReview(train_data[0], reverse_word_index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Textual Data\n",
    "In the lessons on Classifying images, all our images were of the same size (28x28), this wasn't a coincidence. Under the hood Machine Learning makes heavy use of Linear Algebra and the are optimisations we can make if our inputs are the same size/shape.\n",
    "\n",
    "With images (which you will see in the next lesson) we re-size the images to a standard size. With Textual Data we want to make all of our Movie Reviews to be the same length so we use an appraoch called __Padding__ which just inserts some standard value (usually '0') either to the start or the end of the each review to make them the same length. If a review is longer than some size we specify then it is truncated.\n",
    "\n",
    "_Keras_ makes padding very easy using the `keras.preprocessing.sequence.pad_sequences()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will limit our Reviews to the first 256 words and truncate any reviews that are longer than this\n",
    "max_words = 256\n",
    "\n",
    "# We will pad our Training  and Testing reviews so they are all 256 words long and truncate any that are longer\n",
    "# We will add any padding to the end of the sentence.\n",
    "train_data = keras.preprocessing.sequence.pad_sequences(train_data,\n",
    "                                                        value=word_index[\"<PAD>\"],\n",
    "                                                        padding='post',\n",
    "                                                        maxlen=max_words)\n",
    "\n",
    "test_data = keras.preprocessing.sequence.pad_sequences(test_data,\n",
    "                                                       value=word_index[\"<PAD>\"],\n",
    "                                                       padding='post',\n",
    "                                                       maxlen=max_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining our Mode\n",
    "For a basic text classifier, we can use the same layers (_Dense_ and _Flatten_) to learn the relationship between a sentence and its sentiment (Positive or Negative).\n",
    "\n",
    "However that will not perform that well and so what we need is a specialist layer called an __Embedding__ layer. This is a specialised layer that learn relationships between inputs.\n",
    "\n",
    "# TODO: PRovide a better description of Embeddings\n",
    "\n",
    "So when we build our model our _Input Layer_ is an _Embedding_ layer. The IMDB Reviews have over 88,000 distinct \"words\" and this is probably too large for what we need. So we will limit our Embedding Layer to the top 10,000 words only.\n",
    "\n",
    "Our Hidden Layers can be the standard _Dense_ layer (later in this workbook we will use Convolutions but just use Dense layers for now).\n",
    "\n",
    "We are building a model to classify our input as either positive or negative so we only really need a single unit in our _Output Layer_.\n",
    "- why? we can use a single numerical output and assume values close to '0' are Negative reviews and values close to '1' are Positive reviews).\n",
    "\n",
    "Let's define our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input shape is the vocabulary count used for the movie reviews (10,000 words)\n",
    "vocab_size = 10000\n",
    "\n",
    "model = keras.Sequential()\n",
    "# Input Layer\n",
    "model.add(keras.layers.Embedding(input_dim = vocab_size, output_dim=32, input_length=max_words))\n",
    "model.add(keras.layers.Flatten())\n",
    "\n",
    "# Hidden Layers\n",
    "# YOUR CHANGES START HERE\n",
    "# TODO: Decide how many layers you want and how many units on each layer\n",
    "model.add(keras.layers.Dense(units=256, activation=tf.nn.relu))\n",
    "# YOUR CHANGES END HERE\n",
    "\n",
    "# Output Layer\n",
    "model.add(keras.layers.Dense(units=1, activation=tf.nn.sigmoid))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "# Sumarise the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop early if our Validation Loss stagnates\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "# We'll train for some epochs\n",
    "epochs = 20\n",
    "\n",
    "# Train our model\n",
    "history = model.fit(train_data,\n",
    "                    train_labels,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_split = 0.2,\n",
    "                    verbose=2,\n",
    "                   callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate our model\n",
    "We will now evaluate our model against a set of previously unseen reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, val_acc = model.evaluate(test_data, test_labels)\n",
    "\n",
    "print (\"Test Loss:\", val_loss)\n",
    "print (\"Test Accuracy:\", val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lesson5.printLossAndAccuracy(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execise\n",
    "We have a method that will predict a star rating for given review. A really positive review will be predicted as 5 Stars, whereas a really negative review will be predicted as 1 Star.\n",
    "\n",
    "Try some sample reviews by changing the value of _my_review_ and see if the predicted Star rating matches your expectations.\n",
    "\n",
    "Given this system and the specific requirement to estimate a Star rating, what are the risks you should test for?\n",
    "\n",
    "Work in your teams and:\n",
    "- List out some of the risks you would want to test for\n",
    "- Try out some of your ideas out and record any issues you find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a review out\n",
    "my_review = \"ok directing but good casting.\"\n",
    "\n",
    "\n",
    "print(my_review)\n",
    "print(\"\\nPredicted Star Rating (1-5 Stars) is {}\".format(lesson5.predictStarRating(my_review, \n",
    "                                                         model, word_index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Think about your current work, social or other situation and how the use of Text Classification could be used for __good__.\n",
    "\n",
    "Work in your teams and:\n",
    "- Identify possible uses for Text Classification in your context\n",
    "- Think about what data you might need and where you can obtain it from\n",
    "- Consider the Ethical and Social implications of doing this "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification using CNNs (Optional)\n",
    "In the Image Classification Lessons we introduced the idea of Convolutions and Pooling (a CNN) to improve image classification. While Convolutions were originally concieved for Image Processing, we can use the same idea with Textual data (it's all numbers after all!)\n",
    "\n",
    "For images we used `Conv2D()` to define a Convolutional layer that would scan an 2-D image with a 2-D filter. Our text sentences only have one dimention (length) so we use a 1-D Convolutional layer. Keras provides this as `Conv1D()` where rather than provide a 2-D _Kernal_ we specify a 1-D _Kernal_.\n",
    "\n",
    "Aside from these small difference it's very similar to the image processing with a CNN.\n",
    "\n",
    "Let's build one and see if we can improve on our previous model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification using CNN\n",
    "cnn_model = keras.Sequential()\n",
    "\n",
    "# Input Layer\n",
    "cnn_model.add(keras.layers.Embedding(input_dim = vocab_size, \n",
    "                                     output_dim=32, input_length=max_words))\n",
    "\n",
    "# YOUR CHANGES START HERE\n",
    "\n",
    "# Hidden Layers - CNN\n",
    "# TODO: Decide on how many Filters you want to use for each of the Convolutional Layers\n",
    "#       If you want to try additional Convolutional `layers then copy the line below to add addiitonal layers\n",
    "cnn_model.add(keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\", activation=\"relu\"))\n",
    "cnn_model.add(keras.layers.MaxPooling1D(pool_size=2))\n",
    "\n",
    "cnn_model.add(keras.layers.Flatten())\n",
    "# Hidden Dense Layers\n",
    "# TODO: Decide how many units you want in the Dense Layre\n",
    "cnn_model.add(keras.layers.Dense(units=256, activation=tf.nn.relu))\n",
    "\n",
    "\n",
    "# YOUR CHANGES END HERE \n",
    "\n",
    "# Output Layer\n",
    "cnn_model.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))\n",
    "\n",
    "# Compile the model\n",
    "cnn_model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "# Print the Summary of the model\n",
    "cnn_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop early if our Validation Loss stagnates\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "# We'll train for some epochs\n",
    "epochs = 20\n",
    "\n",
    "# Train our model\n",
    "cnn_history = model.fit(train_data,\n",
    "                    train_labels,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_split = 0.2,\n",
    "                    verbose=2,\n",
    "                   callbacks=[early_stop])\n",
    "print(\"Training Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, val_acc = cnn_model.evaluate(test_data, test_labels)\n",
    "\n",
    "print (\"Test Loss:\", val_loss)\n",
    "print (\"Test Accuracy:\", val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lesson5.printLossAndAccuracy(cnn_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execise\n",
    "We have a method that will predict a star rating for given review. A really positive review will be predicted as 5 Stars, whereas a really negative review will be predicted as 1 Star.\n",
    "\n",
    "Try some sample reviews by changing the value of _my_review_ and see if the predicted Star rating matches your expectations.\n",
    "\n",
    "Given this system and the specific requirement to estimate a Star rating, what are the risks you should test for?\n",
    "\n",
    "Work in your teams and:\n",
    "- List out some of the risks you would want to test for\n",
    "- Try out some of your ideas out and record any issues you find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a review out\n",
    "my_review = \"ok directing but good casting.\"\n",
    "\n",
    "\n",
    "print(my_review)\n",
    "print(\"\\nPredicted Star Rating (1-5 Stars) is {}\".format(lesson5.predictStarRating(my_review, \n",
    "                                                         cnn_model, word_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
